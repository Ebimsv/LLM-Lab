seed: 42

# Single flag to control both model and tokenizer
# Set to true to use pretrained model/tokenizer, false to use local/custom ones
use_pretrained: false  # true | false

data:
  source: hf_dataset  # hf_dataset | local_text
  
  # For hf_dataset source:
  dataset_name: Salesforce/wikitext
  dataset_config: wikitext-2-v1
  text_column: text
  
  # For local_text source:
  local_files:
    train: null  # data/raw/wiki.train.tokens
    validation: null  # data/raw/wiki.valid.tokens
    test: null  # data/raw/wiki.test.tokens
  
  # Common settings:
  block_size: 512
  streaming: false
  max_train_samples: null
  max_eval_samples: null

# Model and tokenizer settings
# When use_pretrained=true: uses pretrained_name for both
# When use_pretrained=false: uses tokenizer_dir and builds model from scratch
pretrained_name: openai-community/gpt2  # e.g. gpt2, gpt2-medium, gpt2-large

tokenizer:
  tokenizer_dir: tokenizers/tokenizer_bpe  # Used when use_pretrained=false
  pad_to_eos_if_missing: true

model:
  # Architecture settings (used when use_pretrained=false)
  n_layer: 8
  n_head: 8
  n_embd: 512
  dropout: 0.0

train:
  output_dir: runs/pretrain_gpt2_small
  num_train_epochs: 1
  learning_rate: 3.0e-4
  warmup_steps: 200
  weight_decay: 0.1

  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8

  logging_steps: 50
  eval_steps: 500
  save_steps: 500
  save_total_limit: 3

  fp16: true
  bf16: false

  resume_from_checkpoint: auto  # auto | null | "path/to/checkpoint"

  report_to: none  # none | wandb
  run_name: "pretrain-gpt2-small"

hub:
  push_to_hub: false
  repo_id: null  # "your-username/torch-linguist-pretrain"
  private: true
  hub_strategy: "every_save"  # every_save | end
